{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebf0f4e-a2a9-4b04-9d9e-c320e2b52d80",
   "metadata": {},
   "source": [
    "## NeurAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428f743-f360-4c4e-b356-8fcad83348eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config={\n",
    "    \"n_hidden_layers\": 9,\n",
    "    \"enc_neurons\": 256,\n",
    "    \"dec_layers\": 7,\n",
    "    \"dec_neurons\": 128,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 5000,\n",
    "    \"L\": 10,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"dataset\": \"2azQ1b91cZZ\",\n",
    "    \"voxel_size\": 0.008\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fea4b-cab8-4d61-96b9-d9221b62ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_args = ['-type', 'conf',\n",
    "                '-i', '<path_to_conf_file>',\n",
    "                '-filter', '<room_id>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0311606-11fc-4602-ad5c-610465f1f196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pye57\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from numba import jit\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88595d35-f89d-43a1-a147-cd00ed58c3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class encMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, embedding_dim_p=10, embedding_dim_c=4):   \n",
    "        super(encMLP, self).__init__()\n",
    "\n",
    "        self.embedding_dim_p = embedding_dim_p\n",
    "        self.embedding_dim_c = embedding_dim_c\n",
    "        \n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_p * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_p * 6 + 3 + hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), )\n",
    "\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_c * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
    "\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, p, c):\n",
    "        emb_p = self.positional_encoding(p, self.embedding_dim_p)\n",
    "        emb_c = self.positional_encoding(c, self.embedding_dim_c)\n",
    "        \n",
    "        b1 = self.block1(emb_p)\n",
    "        \n",
    "        b2 = self.block2(torch.cat((b1, emb_p), dim=1))\n",
    "        \n",
    "        b3 = self.block3(torch.cat((b2, emb_c), dim=1))\n",
    "        \n",
    "        b4 = self.block4(b3)\n",
    "        \n",
    "        return b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31453f-b841-46b8-bc1a-475bfe14cd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class decMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = 5\n",
    "        self.output_size = 3\n",
    "        \n",
    "        self.input_layer = nn.Linear(self.input_size, hidden_size)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.input_layer(x))\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34d3a0-0cd7-4178-8f98-9c4f924ae0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cameras_data_from_conf(conf_file_path, scan_filter=None):\n",
    "    working_dir = os.path.dirname(conf_file_path)\n",
    "    cameras_data = {}\n",
    "    header_info = ''\n",
    "\n",
    "    data = open(conf_file_path).readlines()\n",
    "\n",
    "    for line in tqdm(data):\n",
    "\n",
    "        # split by spaces\n",
    "        content = line.split()\n",
    "\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # header information\n",
    "        if content[0] == 'dataset':\n",
    "            title = content[1] + \" dataset\"\n",
    "            header_info += \"\\n#####\" + \"#\" * len(title) + \"#####\"\n",
    "            header_info += \"\\n#    \" + title + \"    #\"\n",
    "            header_info += \"\\n#####\" + \"#\" * len(title) + \"#####\\n\"\n",
    "\n",
    "        if content[0] == 'n_images':\n",
    "            header_info += \"\\nContains \" + content[1] + \" captures\"\n",
    "\n",
    "        if content[0] == 'depth_directory':\n",
    "            depth_dir = content[1]\n",
    "            header_info += \"\\nDepth directory is \" + depth_dir\n",
    "\n",
    "        if content[0] == 'color_directory':\n",
    "            rgb_dir = content[1]\n",
    "            header_info += \"\\nRGB directory is \" + rgb_dir\n",
    "\n",
    "        # scan data\n",
    "        if content[0] == 'intrinsics_matrix':\n",
    "            intrinsics_list = content[1:]\n",
    "            # intrinsics matrix\n",
    "            K = np.zeros((3, 3))\n",
    "            K[0, :] = intrinsics_list[0:3]\n",
    "            K[1, :] = intrinsics_list[3:6]\n",
    "            K[2, :] = intrinsics_list[6:9]\n",
    "\n",
    "        if content[0] == 'scan':\n",
    "            depth_file_name = content[1]\n",
    "            rgb_file_name = content[2]\n",
    "            extrinsics_list = content[3:]\n",
    "\n",
    "            if scan_filter:\n",
    "                if depth_file_name.split('_')[0] not in scan_filter:\n",
    "                    continue\n",
    "\n",
    "            scan_depth_image = o3d.io.read_image(working_dir + '/' + depth_dir + '/' + depth_file_name)\n",
    "            # 0.25mm per unit, divide by 4000 to get meters\n",
    "            scan_depth_image = np.asarray(scan_depth_image, dtype=np.float32) / 4000\n",
    "\n",
    "            image = o3d.io.read_image(working_dir + '/' + rgb_dir + '/' + rgb_file_name)\n",
    "            image = np.asarray(image)\n",
    "\n",
    "            height, width, _ = image.shape\n",
    "\n",
    "            # extrinsics matrix\n",
    "            cam_matrix = np.zeros((4, 4))\n",
    "            cam_matrix[0, :] = extrinsics_list[0:4]\n",
    "            cam_matrix[1, :] = extrinsics_list[4:8]\n",
    "            cam_matrix[2, :] = extrinsics_list[8:12]\n",
    "            cam_matrix[3, :] = extrinsics_list[12:16]\n",
    "\n",
    "            opengl_to_opencv = np.zeros((4, 4))\n",
    "            opengl_to_opencv[0, 0] = 1\n",
    "            opengl_to_opencv[1, 1] = -1\n",
    "            opengl_to_opencv[2, 2] = -1\n",
    "            opengl_to_opencv[3, 3] = 1\n",
    "            cam_matrix = np.dot(cam_matrix, opengl_to_opencv)\n",
    "\n",
    "            cameras_data[rgb_file_name.split('.')[0]] = {'extrinsics': cam_matrix,\n",
    "                                                         'intrinsics': K,\n",
    "                                                         'height': height,\n",
    "                                                         'width': width,\n",
    "                                                         'imageBGR': cv2.cvtColor(image, cv2.COLOR_RGB2BGR),\n",
    "                                                         'scan_depth_image': scan_depth_image\n",
    "                                                         }\n",
    "    print(header_info)\n",
    "    return cameras_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb58bf7-c36f-4ab7-b4dc-af200c1d7f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cameras_data(e57):\n",
    "    cameras_data = {}\n",
    "\n",
    "    imf = e57.image_file\n",
    "    root = imf.root()\n",
    "\n",
    "    for image_idx, image2D in enumerate(tqdm(root['images2D'])):\n",
    "        # get extrinsic matrix\n",
    "        tx = image2D['pose']['translation']['x'].value()\n",
    "        ty = image2D['pose']['translation']['y'].value()\n",
    "        tz = image2D['pose']['translation']['z'].value()\n",
    "\n",
    "        t = np.array([tx, ty, tz])\n",
    "\n",
    "        rx = image2D['pose']['rotation']['x'].value()\n",
    "        ry = image2D['pose']['rotation']['y'].value()\n",
    "        rz = image2D['pose']['rotation']['z'].value()\n",
    "        rw = image2D['pose']['rotation']['w'].value()\n",
    "\n",
    "        r = Rotation.from_quat(np.array([rx, ry, rz, rw]))\n",
    "\n",
    "        cam_matrix = np.zeros((4, 4))\n",
    "        cam_matrix[3, 3] = 1\n",
    "        cam_matrix[:-1, -1] = t\n",
    "        cam_matrix[:3, :3] = r.as_matrix()\n",
    "\n",
    "        opengl_to_opencv = np.zeros((4, 4))\n",
    "        opengl_to_opencv[0, 0] = 1\n",
    "        opengl_to_opencv[1, 1] = -1\n",
    "        opengl_to_opencv[2, 2] = -1\n",
    "        opengl_to_opencv[3, 3] = 1\n",
    "        cam_matrix = np.dot(cam_matrix, opengl_to_opencv)\n",
    "\n",
    "        # get intrinsic matrix\n",
    "        pinhole = image2D['pinholeRepresentation']\n",
    "\n",
    "        focal_length = pinhole['focalLength'].value()\n",
    "        pixel_height = pinhole['pixelHeight'].value()\n",
    "        pixel_width = pinhole['pixelWidth'].value()\n",
    "        principal_point_x = pinhole['principalPointX'].value()\n",
    "        principal_point_y = pinhole['principalPointY'].value()\n",
    "\n",
    "        K = np.zeros((3, 3))\n",
    "        K[2, 2] = 1\n",
    "        K[0, 0] = focal_length / pixel_width\n",
    "        K[1, 1] = focal_length / pixel_height\n",
    "        K[0, 2] = principal_point_x\n",
    "        K[1, 2] = principal_point_y\n",
    "\n",
    "        # get picture from blob\n",
    "        jpeg_image = pinhole['jpegImage']\n",
    "        jpeg_image_data = np.zeros(shape=jpeg_image.byteCount(), dtype=np.uint8)\n",
    "        jpeg_image.read(jpeg_image_data, 0, jpeg_image.byteCount())\n",
    "        image = cv2.imdecode(jpeg_image_data, cv2.IMREAD_COLOR)\n",
    "\n",
    "        height, width, channels = image.shape\n",
    "\n",
    "        ##############################################\n",
    "        # generate depth image from laser scan\n",
    "        pc_data = e57.read_scan(image_idx // 6)\n",
    "        pts = [pc_data['cartesianX'], pc_data['cartesianY'], pc_data['cartesianZ']]\n",
    "        scan_pcd = o3d.geometry.PointCloud()\n",
    "        scan_pcd.points = o3d.utility.Vector3dVector(np.vstack(pts).transpose())\n",
    "\n",
    "        renderer = o3d.visualization.rendering.OffscreenRenderer(width, height)\n",
    "        mtl = o3d.visualization.rendering.MaterialRecord()\n",
    "        mtl.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "        mtl.shader = \"defaultUnlit\"\n",
    "        renderer.scene.add_geometry(\"textured_mesh\", scan_pcd, mtl)\n",
    "        o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "        renderer.setup_camera(o3d_intrinsics, np.linalg.inv(cam_matrix))\n",
    "        scan_depth_image = np.array(renderer.render_to_depth_image(z_in_view_space=True))\n",
    "\n",
    "        cameras_data[image_idx] = {'extrinsics': cam_matrix,\n",
    "                                   'intrinsics': K,\n",
    "                                   'height': height,\n",
    "                                   'width': width,\n",
    "                                   'imageBGR': image,\n",
    "                                   'scan_depth_image': scan_depth_image,\n",
    "                                   'scan_idx': image_idx // 6\n",
    "                                   }\n",
    "\n",
    "    return cameras_data\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def project_to_camera(intrinsic_matrix, distortion, width, height, pts):\n",
    "    _, n_pts = pts.shape\n",
    "\n",
    "    pixs = np.zeros((2, n_pts), dtype=np.float64)\n",
    "\n",
    "    k1, k2, p1, p2, k3 = distortion\n",
    "    # fx, _, cx, _, fy, cy, _, _, _ = intrinsic_matrix\n",
    "    # print('intrinsic=\\n' + str(intrinsic_matrix))\n",
    "    fx = intrinsic_matrix[0, 0]\n",
    "    fy = intrinsic_matrix[1, 1]\n",
    "    cx = intrinsic_matrix[0, 2]\n",
    "    cy = intrinsic_matrix[1, 2]\n",
    "\n",
    "    x = pts[0, :]\n",
    "    y = pts[1, :]\n",
    "    z = pts[2, :]\n",
    "\n",
    "    xl = np.divide(x, z)\n",
    "    yl = np.divide(y, z)\n",
    "    r2 = xl ** 2 + yl ** 2\n",
    "    xll = xl * (1 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3) + 2 * p1 * xl * yl + p2 * (r2 + 2 * xl ** 2)\n",
    "    yll = yl * (1 + k1 * r2 + k2 * r2 ** 2 + k3 * r2 ** 3) + p1 * (r2 + 2 * yl ** 2) + 2 * p2 * xl * yl\n",
    "    pixs[0, :] = fx * xll + cx\n",
    "    pixs[1, :] = fy * yll + cy\n",
    "\n",
    "    # compute mask of valid projections\n",
    "    valid_z = z > 0\n",
    "    valid_xpix = np.logical_and(pixs[0, :] >= 0, pixs[0, :] < width)\n",
    "    valid_ypix = np.logical_and(pixs[1, :] >= 0, pixs[1, :] < height)\n",
    "    valid_pixs = np.logical_and(valid_z, np.logical_and(valid_xpix, valid_ypix))\n",
    "    return pixs, valid_pixs\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def filter_points_in_view(range_sparse, dists, width, height, indexes):\n",
    "    depth_image = np.full((height, width), fill_value=-1, dtype=np.float64)\n",
    "    result = np.full((height, width), fill_value=-1, dtype=np.int32)\n",
    "\n",
    "    for idx, dist in enumerate(dists):\n",
    "        x, y = range_sparse[idx]\n",
    "\n",
    "        # if a point closer to the camera has been projected to this pixel\n",
    "        if depth_image[y, x] != -1 and depth_image[y, x] < dist:\n",
    "            continue\n",
    "\n",
    "        depth_image[y, x] = dist\n",
    "\n",
    "        result[y, x] = indexes[idx]\n",
    "\n",
    "    return depth_image, result\n",
    "\n",
    "\n",
    "def get_viewpoint_depth(pts_hom, cam_matrix, K, width, height):\n",
    "    \"\"\"\n",
    "    Get a depth image with points visible from a camera viewpoint and their corresponding indexes.\n",
    "\n",
    "    Parameters:\n",
    "    - pts_hom (array-like): A vector of 3D points to project onto the camera view.\n",
    "    - cam_matrix (array-like): The extrinsic matrix of the camera.\n",
    "    - K (array-like): The intrinsic matrix of the camera.\n",
    "    - width (int): The width of the output depth image (in pixels).\n",
    "    - height (int): The height of the output depth image (in pixels).\n",
    "\n",
    "    Returns:\n",
    "    - depth_image (array-like): A matrix with depth values at each (x, y) pixel location.\n",
    "    - point_indexes (array-like): A matrix with indices of the corresponding points at each (x, y) pixel location.\n",
    "\n",
    "    Notes:\n",
    "    - Ensure that the input points in 'pts_hom' are in a format compatible with the camera's coordinate system and that\n",
    "    'cam_matrix' and 'K' are properly calibrated camera matrices.\n",
    "    - The 'depth_image' will contain depth values for visible points at each pixel location.\n",
    "    - The 'point_indexes' will contain the index of the corresponding point from 'pts_hom' at each pixel location.\n",
    "    \"\"\"\n",
    "    # project points to camera coordinate system\n",
    "    pts_in_camera = np.dot(np.linalg.inv(cam_matrix), pts_hom)\n",
    "\n",
    "    pixels, valid_pixels = project_to_camera(K,\n",
    "                                             np.array([0, 0, 0, 0, 0]),\n",
    "                                             width,\n",
    "                                             height,\n",
    "                                             pts_in_camera)\n",
    "\n",
    "    range_sparse = np.transpose(np.vstack((pixels[0, valid_pixels],\n",
    "                                           pixels[1, valid_pixels])).astype(int))\n",
    "    dists = pts_in_camera[2, valid_pixels]\n",
    "    indexes = np.where(valid_pixels)[0]\n",
    "\n",
    "    depth_image, point_indexes = filter_points_in_view(range_sparse, dists, width, height, indexes)\n",
    "\n",
    "    return depth_image, point_indexes\n",
    "\n",
    "\n",
    "def compute_pairwise_mappings(idxs_per_camera, sparse_per_camera):\n",
    "    camera_mappings = {}\n",
    "    for i in tqdm(range(len(idxs_per_camera) - 1)):\n",
    "        for j in range(i + 1, len(idxs_per_camera)):\n",
    "            # find points of cam i in cam j\n",
    "            np_mappings = np.intersect1d(idxs_per_camera[i], idxs_per_camera[j], return_indices=True)\n",
    "\n",
    "            # list of corresponding indices for i and j\n",
    "            mappings_i = sparse_per_camera[i][np_mappings[1]]\n",
    "            mappings_j = sparse_per_camera[j][np_mappings[2]]\n",
    "\n",
    "            # store mappings in data structures\n",
    "            camera_mappings[(i, j)] = [mappings_i, mappings_j]\n",
    "    return camera_mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ae966-5753-4c1e-9642-259fe72c148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsdf(cameras_data, voxel_size):\n",
    "    \n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(voxel_length=voxel_size,\n",
    "                                                          sdf_trunc=10 * voxel_size,\n",
    "                                                          color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "\n",
    "    for camera in tqdm(cameras_data):\n",
    "        image = o3d.geometry.Image(cv2.cvtColor(cameras_data[camera]['imageBGR'], cv2.COLOR_BGR2RGB))\n",
    "        scan_depth_image = o3d.geometry.Image(cameras_data[camera]['scan_depth_image'] * 1000)\n",
    "\n",
    "        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(image,\n",
    "                                                                  scan_depth_image,\n",
    "                                                                  depth_trunc=10.0,\n",
    "                                                                  convert_rgb_to_intensity=False)\n",
    "\n",
    "        K = cameras_data[camera]['intrinsics']\n",
    "        width = cameras_data[camera]['width']\n",
    "        height = cameras_data[camera]['height']\n",
    "        o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "        volume.integrate(rgbd,\n",
    "                         o3d_intrinsics,\n",
    "                         np.linalg.inv(cameras_data[camera]['extrinsics']))\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff7990-4538-4121-bd0e-7b4ef69d8f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355526f-b650-4240-8ae5-b93a0c04cd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-type', '--input_type', required=True, type=str, choices=['e57', 'conf'],\n",
    "                help=\"Type of input files.\\n\"\n",
    "                     \"'e57' -  E57 file contains all the information.\\n\"\n",
    "                     \"'conf' - Config file containing camera parameters and path to depth and rgb.\\n\")\n",
    "ap.add_argument('-i', '--input_path', required=True, type=str,\n",
    "                help=\"Path to the input file.\")\n",
    "ap.add_argument('-cache', '--cache_path', required=False, help=\"Path for cache directory.\")\n",
    "ap.add_argument('-filter', '--region_filter', required=False, type=int, nargs='+',\n",
    "                help=\"List of region ids to keep.\")\n",
    "args = vars(ap.parse_args(command_args))\n",
    "\n",
    "if args['input_type'] == 'e57':\n",
    "    print(\"\\nLoading E57...\")\n",
    "    e57 = pye57.E57(args['input_path'])\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    print(\"\\nReading point cloud scans...\")\n",
    "    for scan_idx in tqdm(range(e57.scan_count)):\n",
    "        pc_data = e57.read_scan(scan_idx, colors=False)\n",
    "    \n",
    "        pc_points = [pc_data['cartesianX'], pc_data['cartesianY'], pc_data['cartesianZ']]\n",
    "        tmp_pcd = o3d.geometry.PointCloud()\n",
    "        tmp_pcd.points = o3d.utility.Vector3dVector(np.vstack(pc_points).transpose())\n",
    "    \n",
    "        if args['cache_path'] and os.path.isfile(args['cache_path'] + '/normals/' + str(scan_idx) + '.npy'):\n",
    "            # load normals\n",
    "            loaded_array = np.load(args['cache_path'] + '/normals/' + str(scan_idx) + '.npy')\n",
    "            tmp_pcd.normals = o3d.utility.Vector3dVector(loaded_array)\n",
    "        else:\n",
    "            # estimate normals and flip using scanner position\n",
    "            tmp_pcd.estimate_normals()\n",
    "            header = e57.get_header(scan_idx)\n",
    "            cam_matrix = np.zeros((4, 4))\n",
    "            cam_matrix[3, 3] = 1\n",
    "            cam_matrix[:-1, -1] = header.translation\n",
    "            cam_matrix[:3, :3] = header.rotation_matrix\n",
    "            tmp_pcd.orient_normals_towards_camera_location(camera_location=cam_matrix[:3, 3])\n",
    "    \n",
    "            # cache normals\n",
    "            if args['cache_path']:\n",
    "                if not os.path.exists(args['cache_path']):\n",
    "                    os.makedirs(args['cache_path'])\n",
    "                    os.makedirs(args['cache_path'] + '/normals/')\n",
    "                else:\n",
    "                    if not os.path.exists(args['cache_path'] + '/normals/'):\n",
    "                        os.makedirs(args['cache_path'] + '/normals/')\n",
    "    \n",
    "                    np.save(args['cache_path'] + '/normals/' + str(scan_idx) + '.npy',\n",
    "                            np.array(tmp_pcd.normals))\n",
    "        pcd += tmp_pcd\n",
    "        # break\n",
    "    \n",
    "    pcd = pcd.voxel_down_sample(voxel_size=config[\"voxel_size\"])\n",
    "\n",
    "    # load camera data from cache if available\n",
    "    print(\"\\nLoading cameras data...\")\n",
    "    if args['cache_path'] and os.path.isfile(args['cache_path'] + '/camera_data.pickle'):\n",
    "        with open(args['cache_path'] + '/camera_data.pickle', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "    else:\n",
    "        cameras_data = get_cameras_data(e57)\n",
    "    \n",
    "        # cache camera data\n",
    "        if args['cache_path']:\n",
    "            with open(args['cache_path'] + '/camera_data.pickle', 'xb') as handle:\n",
    "                pickle.dump(cameras_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"\\nDone\")\n",
    "\n",
    "elif args['input_type'] == 'conf':\n",
    "    scan_filter = []\n",
    "    if args['region_filter']:\n",
    "        segmentation_path = os.path.dirname(args['input_path']) + '/panorama_to_region.txt'\n",
    "        if not os.path.isfile(segmentation_path):\n",
    "            print(\"\\nERROR: \" + segmentation_path + \" not found!\")\n",
    "            print(\"Region segmentation is required to use filter. Exiting...\")\n",
    "            exit()\n",
    "\n",
    "        seg_data = open(segmentation_path).readlines()\n",
    "\n",
    "        print(\"\\nProcessing region segmentation...\")\n",
    "        for line in tqdm(seg_data):\n",
    "\n",
    "            # split by spaces\n",
    "            content = line.split()\n",
    "\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            if int(content[2]) in args['region_filter']:\n",
    "                scan_filter.append(content[1])\n",
    "\n",
    "    print(\"\\nLoading camera data...\")\n",
    "    cameras_data = get_cameras_data_from_conf(args['input_path'], scan_filter)\n",
    "\n",
    "    print(\"\\nIntegrating data in TSDF...\")\n",
    "    tsdf = create_tsdf(cameras_data, config[\"voxel_size\"])\n",
    "\n",
    "    print(\"\\nExtracting point cloud...\")\n",
    "    pcd = tsdf.extract_point_cloud()\n",
    "\n",
    "    # file_name = \"voxel_size_\" + str(config[\"voxel_size\"]) + \"_batch_\" + str(config[\"batch_size\"]) + \"_epochs_\" + str(config[\"epochs\"]) + \"_L_\" + str(config[\"L\"]) + \".ply\"\n",
    "    # o3d.io.write_point_cloud(file_name, pcd)\n",
    "\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772482af-1e5f-496a-8572-ffc60eb0db67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e614d-65ff-4be7-b36e-524494d0a4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # explicit filter\n",
    "# image_list = [11, 41]\n",
    "# cameras_data = {image_list.index(key): data[key] for key in image_list}\n",
    "\n",
    "sparse_per_camera = np.empty(len(cameras_data), dtype=object)\n",
    "idxs_per_camera = np.empty(len(cameras_data), dtype=object)\n",
    "\n",
    "cam_pos = np.empty(len(cameras_data), dtype=object)\n",
    "vector_src = np.empty(len(cameras_data), dtype=object)\n",
    "vector_tgt = np.empty(len(cameras_data), dtype=object)\n",
    "\n",
    "print(\"\\nProjecting points...\")\n",
    "for cam_idx, camera in enumerate(tqdm(cameras_data)):\n",
    "    \n",
    "    # project points to camera\n",
    "    pts_hom = np.array(pcd.points).T\n",
    "    pts_hom = np.vstack([pts_hom, np.ones(len(pts_hom[1]), dtype=float)])\n",
    "    depth_image, point_indexes = get_viewpoint_depth(pts_hom,\n",
    "                                                     cameras_data[camera]['extrinsics'],\n",
    "                                                     cameras_data[camera]['intrinsics'],\n",
    "                                                     cameras_data[camera]['width'],\n",
    "                                                     cameras_data[camera]['height'])\n",
    "\n",
    "    # z-buffering filter\n",
    "    zbuffer_invalid = np.where(abs(depth_image - cameras_data[camera]['scan_depth_image']) > 0.06)\n",
    "    point_indexes[zbuffer_invalid] = -1\n",
    "\n",
    "    # get the row,col of points that were actually projected\n",
    "    sparse = np.array(list(zip(np.where(point_indexes != -1)))).T.reshape(-1, 2)\n",
    "    sparse_per_camera[cam_idx] = sparse\n",
    "    \n",
    "    # keep index of 3D points that were actually projected\n",
    "    idxs_per_camera[cam_idx] = point_indexes[point_indexes != -1]\n",
    "    \n",
    "    # get colour data\n",
    "    img_bgr = cameras_data[camera]['imageBGR']\n",
    "\n",
    "    vector_src[cam_idx] = np.array(pcd.points)[idxs_per_camera[cam_idx]]    # X Y Z\n",
    "    \n",
    "    if len(vector_src[cam_idx]) == 0:\n",
    "        cam_pos[cam_idx] = np.array(pcd.points)[idxs_per_camera[cam_idx]]\n",
    "    else:\n",
    "        pos = cameras_data[camera]['extrinsics'][:3, 3]\n",
    "        cam_pos[cam_idx] = np.vstack([pos] * len(vector_src[cam_idx]))    # Tx Ty Tz\n",
    "\n",
    "    vector_tgt[cam_idx] = img_bgr[sparse[:, 0], sparse[:, 1]] / 255    # B G R\n",
    "    \n",
    "vector_src = np.concatenate(vector_src)\n",
    "cam_pos = np.concatenate(cam_pos)\n",
    "vector_tgt = np.concatenate(vector_tgt)\n",
    "\n",
    "# z-score normalization\n",
    "input_mean = np.mean(vector_src, axis=0)\n",
    "input_std = np.std(vector_src, axis=0)\n",
    "norm_vector_src = (vector_src - input_mean) / input_std\n",
    "\n",
    "cam_pos_mean = np.mean(cam_pos, axis=0)\n",
    "cam_pos_std = np.std(cam_pos, axis=0)\n",
    "norm_cam_pos = (cam_pos - cam_pos_mean) / cam_pos_std\n",
    "\n",
    "norm_inputs = torch.tensor(np.hstack([norm_vector_src, norm_cam_pos]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211e4a8-17f3-4c94-b83c-23a96e277f78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training scene encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb43d4-a41b-45be-9eb2-36c67d34b83b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "enc_dataset = TensorDataset(norm_inputs, torch.tensor(vector_tgt))\n",
    "dec_trainloader = DataLoader(enc_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "mlp = encMLP(config[\"enc_neurons\"], config[\"L\"])\n",
    "\n",
    "mlp = mlp.to(\"cuda\", dtype=torch.float32)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} / {config['epochs']}:\")\n",
    "\n",
    "    cost = 0.0\n",
    "    n_batches = 0\n",
    "    for batch in tqdm(dec_trainloader):\n",
    "        inputs, targets = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = mlp(inputs[:, :3].to(\"cuda\", dtype=torch.float32), \n",
    "                      inputs[:, -3:].to(\"cuda\", dtype=torch.float32))\n",
    "\n",
    "        loss = loss_function(outputs, targets.to(\"cuda\", dtype=torch.float32))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss.item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        # wandb.log({\"Batch\": n_batches * (epoch + 1),\n",
    "        #            \"Loss\": loss.item()})\n",
    "\n",
    "    print(\"Average cost for Epoch: \" + str(cost / n_batches))\n",
    "    # wandb.log({\"Epoch\": epoch,        \n",
    "    #            \"Average Loss\": cost / n_batches})\n",
    "\n",
    "    torch.save(mlp, 'v_d_encoder.pth')\n",
    "    \n",
    "end = time.time()\n",
    "seconds = int(end - start)\n",
    "print(\"\\n---------------------------------\")\n",
    "print(f\"\\nCompleted in \"f\"{seconds // 3600:02d}:\"f\"{(seconds // 60) % 60:02d}:\"f\"{seconds % 60:02d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc6445-97f0-4a79-a4ea-ad3845d1ef7b",
   "metadata": {},
   "source": [
    "### Creating encoder vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b04980-404f-404e-a67b-5cc7b988018c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = torch.load('v_d_encoder.pth')\n",
    "mlp.to(\"cuda\", dtype=torch.float32)\n",
    "\n",
    "norm_src_test = (np.array(pcd.points) - input_mean) / input_std\n",
    "\n",
    "for idx, camera_id in enumerate(cameras_data):\n",
    "\n",
    "    if not idx == 42:\n",
    "        continue\n",
    "    \n",
    "    cam_matrix = cameras_data[camera_id]['extrinsics']\n",
    "    K = cameras_data[camera_id]['intrinsics']\n",
    "    width, height = cameras_data[camera_id]['width'], cameras_data[camera_id]['height']\n",
    "    \n",
    "    pos = copy.deepcopy(cam_matrix[:3, 3])\n",
    "    \n",
    "    cam_pos_test = np.vstack([pos] * len(norm_src_test))\n",
    "    norm_cam_pos_test = (cam_pos_test - cam_pos_mean) / cam_pos_std\n",
    "    \n",
    "    norm_inputs_test =  torch.tensor(np.hstack([norm_src_test, norm_cam_pos_test]))\n",
    "    # norm_inputs_test =  torch.tensor(norm_src_test)\n",
    "    \n",
    "    test_batch_size = 10000\n",
    "    test_dataset = TensorDataset(norm_inputs_test)\n",
    "    testloader = DataLoader(test_dataset, batch_size=test_batch_size, num_workers=4)\n",
    "    \n",
    "    mlp.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in tqdm(testloader):\n",
    "    \n",
    "            batch_predictions = mlp(batch[0][:, :3].to(\"cuda\", dtype=torch.float32), \n",
    "                                    batch[0][:, -3:].to(\"cuda\", dtype=torch.float32))\n",
    "    \n",
    "            all_predictions.append(batch_predictions)\n",
    "    \n",
    "    final_predictions = torch.cat(all_predictions, dim=0).to(\"cpu\")\n",
    "    \n",
    "    pcd.colors = o3d.utility.Vector3dVector(final_predictions.numpy()[:, ::-1])\n",
    "    \n",
    "    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.1)\n",
    "    sphere.translate(pos)\n",
    "    sphere.paint_uniform_color([1,0,0])\n",
    "    o3d.visualization.draw_geometries([pcd, sphere])\n",
    "    \n",
    "    # # render image\n",
    "    # renderer = o3d.visualization.rendering.OffscreenRenderer(width, height)\n",
    "    # mtl = o3d.visualization.rendering.MaterialRecord()\n",
    "    # mtl.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "    # mtl.shader = \"defaultUnlit)\"\n",
    "    # renderer.scene.add_geometry(\"textured_mesh\", pcd, mtl)\n",
    "    # eye = np.array([ -4.6261574364973619, 0.64065648187804303, 1.6956380986471207 ])\n",
    "    # center = eye - np.array([ -0.67223291846430644, -0.73171576421941253, 0.11267184087322939 ])\n",
    "    # up = np.array([ 0.076496362433848372, 0.082724737828267197, 0.99363218762559902 ])\n",
    "    # renderer.setup_camera(90.0, center, eye, up)\n",
    "    # view_dep_image = np.array(renderer.render_to_image())\n",
    "\n",
    "    # cv2.imshow('view_dep_image', cv2.cvtColor(view_dep_image, cv2.COLOR_RGB2BGR))\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    \n",
    "    file_name = \"voxel_size_\" + str(config[\"voxel_size\"]) + \"_batch_\" + str(config[\"batch_size\"]) + \"_epochs_\" + str(config[\"epochs\"]) + \"_L_\" + str(config[\"L\"]) + \".ply\"\n",
    "    o3d.io.write_point_cloud(file_name, pcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d11cc9-3ca0-41b5-b560-eabc939a331e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scene decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6234dd7-2fa7-42c8-b57a-17ed073dba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = torch.load('v_d_encoder.pth')\n",
    "mlp.to(\"cuda\", dtype=torch.float32)\n",
    "file_out_name = os.path.basename(args['input_path'].split('.')[0])\n",
    "reference_cam = 24\n",
    "batch_size = 1000\n",
    "epochs = 20\n",
    "\n",
    "for camera_idx, camera_key in enumerate(cameras_data):\n",
    "\n",
    "    print(\"Camera \" + str(camera_idx))\n",
    "\n",
    "    camera = cameras_data[camera_key]\n",
    "\n",
    "    # depth_image = camera['scan_depth_image']\n",
    "    # height, width = camera['height'], camera['width']\n",
    "    # K = camera['intrinsics']\n",
    "    # extrinsics = camera['extrinsics']\n",
    "    # o3d_intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "    # point_cloud = o3d.geometry.PointCloud.create_from_depth_image(o3d.geometry.Image(depth_image), o3d_intrinsics, np.linalg.inv(extrinsics))\n",
    "    # point_cloud.remove_non_finite_points()\n",
    "\n",
    "    point_cloud = pcd\n",
    "\n",
    "    # project points to camera\n",
    "    pts_hom = np.array(point_cloud.points).T\n",
    "    pts_hom = np.vstack([pts_hom, np.ones(len(pts_hom[1]), dtype=float)])\n",
    "    depth_image, point_indexes = get_viewpoint_depth(pts_hom,\n",
    "                                                     camera['extrinsics'],\n",
    "                                                     camera['intrinsics'],\n",
    "                                                     camera['width'],\n",
    "                                                     camera['height'])\n",
    "    \n",
    "    # Z buffering filter\n",
    "    zbuffer_invalid = np.where(abs(depth_image - camera['scan_depth_image']) > 0.06)\n",
    "    point_indexes[zbuffer_invalid] = -1\n",
    "\n",
    "    # print(len(point_indexes[point_indexes != -1]))\n",
    "\n",
    "    # check if there are enough valid points\n",
    "    if len(point_indexes[point_indexes != -1]) < 100:\n",
    "        print(\"Not enough geometric data...\")\n",
    "\n",
    "    # get the row,col of points that were actually projected\n",
    "    sparse = np.array(list(zip(np.where(point_indexes != -1)))).T.reshape(-1, 2)\n",
    "\n",
    "    # get colour data\n",
    "    img_bgr = camera['imageBGR']\n",
    "\n",
    "    enc_vector_src = np.array(point_cloud.points)[point_indexes[point_indexes != -1]]    # X Y Z\n",
    "    enc_vector_tgt = img_bgr[sparse[:, 0], sparse[:, 1]] / 255         # B G R\n",
    "\n",
    "    vector_src_test =  torch.tensor((enc_vector_src - input_mean) / input_std)\n",
    "\n",
    "    # camera position\n",
    "    pos = cameras_data[list(cameras_data.keys())[reference_cam]]['extrinsics'][:3, 3]\n",
    "    \n",
    "    cam_pos_test = np.vstack([pos] * len(vector_src_test))\n",
    "    norm_cam_pos_test = (cam_pos_test - cam_pos_mean) / cam_pos_std\n",
    "    norm_inputs_test =  torch.tensor(np.hstack([vector_src_test, norm_cam_pos_test]))\n",
    "\n",
    "    test_batch_size = 10000\n",
    "    test_dataset = TensorDataset(norm_inputs_test)\n",
    "    testloader = DataLoader(test_dataset, batch_size=test_batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    mlp.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"Getting data from scene encoder...\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(testloader):\n",
    "\n",
    "            batch_predictions = mlp(batch[0][:, :3].to(\"cuda\", dtype=torch.float32), \n",
    "                                    batch[0][:, -3:].to(\"cuda\", dtype=torch.float32))\n",
    "\n",
    "            all_predictions.append(batch_predictions)\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions, dim=0).to(\"cpu\")\n",
    "\n",
    "    # now train another MLP to learn RGBXY -> RGB\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # BGRYX vectors\n",
    "    BGRYX_src = np.hstack((enc_vector_tgt,\n",
    "                            sparse[:, 0:1] / camera['height'],\n",
    "                            sparse[:, 1:2] / camera['width']))\n",
    "\n",
    "    tensor_x = torch.Tensor(BGRYX_src)\n",
    "    tensor_y = final_predictions\n",
    "\n",
    "    my_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    trainloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # initialize scene decoder\n",
    "    decoder = decMLP(config[\"dec_neurons\"], config[\"dec_layers\"])\n",
    "    \n",
    "    decoder = decoder.to(\"cuda\", dtype=torch.float32)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    losses = []\n",
    "    print(\"\\nTraining scene decoder...\")\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} / {epochs}:\")\n",
    "\n",
    "        cost = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for data in tqdm(trainloader):\n",
    "            inputs, targets = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_t = inputs.to(\"cuda\", dtype=torch.float32)\n",
    "\n",
    "            outputs = decoder(batch_t)\n",
    "\n",
    "            loss = loss_function(outputs, targets.to(\"cuda\", dtype=torch.float32))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            cost += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        print(\"Average cost for Epoch: \" + str(cost / n_batches))\n",
    "        losses.append(cost / n_batches)\n",
    "\n",
    "    # # Plotting the loss graph\n",
    "    # plt.plot(range(1, epochs + 1), losses, label='Training Loss')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Training Loss Over Epochs')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    #########################################################\n",
    "    # testing the scene decoder\n",
    "\n",
    "    source_test_img = camera['imageBGR'] / 255\n",
    "    height, width = camera['height'], camera['width']\n",
    "    vector_src_test = np.hstack((source_test_img.reshape(-1, 3),\n",
    "                                 np.indices((height, width)).reshape(2, -1).T / np.array([height, width])))\n",
    "\n",
    "    tensor_x_test = torch.tensor(vector_src_test)\n",
    "\n",
    "    f_batch_size = 10000\n",
    "    f_test_dataset = TensorDataset(tensor_x_test)\n",
    "    f_testloader = DataLoader(f_test_dataset, batch_size=f_batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    decoder.eval()\n",
    "\n",
    "    print(\"\\nGenerating enhanced texture...\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        all_predictions = []\n",
    "\n",
    "        for batch in tqdm(f_testloader):\n",
    "\n",
    "            batch_t = batch[0].to(\"cuda\", dtype=torch.float32)\n",
    "\n",
    "            predictions = decoder(batch_t)\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions, dim=0).to(\"cpu\")\n",
    "\n",
    "    synthetic_img = final_predictions.view((height, width, 3)).numpy()\n",
    "\n",
    "    cv2.imwrite(file_out_name + '_' + str(camera_idx) + '.png', (synthetic_img * 255).astype(int))\n",
    "\n",
    "    plt.imshow(synthetic_img[:, :, ::-1])\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199f53f-38b1-407a-bfa3-80db44fb56c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
